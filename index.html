<!DOCTYPE HTML>
<html lang="en">
<head>
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-66DNLPJ6PY"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-66DNLPJ6PY');
  </script>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Jiyao Zhang</title>
  <meta name="author" content="Jon Barron">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<!-- bib hide -->
<script type="text/javascript">
  function hideshow(which){
  if (!document.getElementById)
  return
  if (which.style.display=="block")
  which.style.display="none"
  else
  which.style.display="block"
  }
</script>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <p class="name" style="text-align: center;">
                Jiyao Zhang
              </p>
              <p> I am a first-year Ph.D. candidate in the <a href="https://cfcs.pku.edu.cn/english">Center on Frontiers of Computing Studies (CFCS)</a> at the <a href="https://cs.pku.edu.cn/English/Home.htm">School of Computer Science</a>, <a href="https://english.pku.edu.cn/">Peking University</a>, advised by <a href="https://zsdonghao.github.io/">Prof. Hao Dong</a>. 
                  I'm also a research intern at <a href= "https://www.baai.ac.cn/english.html">Beijing Academy of Artificial Intelligence (BAAI)</a>.
              </p>
              <p style="text-align:center">
                <a href="mailto:jiyaozhang@stu.pku.edu.cn">Email</a> &nbsp;/&nbsp;
                <a href="https://scholar.google.com/citations?user=nf1Q7P4AAAAJ&hl">Google Scholar</a>
              </p>
            </td>
            <td style="padding:2.5%;width:25%;max-width:40%">
              <a href="images/jiyao_circle.jpg"><img style="width:95%;max-width:95%" alt="profile photo" src="images/jiyao_circle.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <h2>Research</h2>
              <p>
                My research interests lie in 3D computer vision and robotics. Much of my research is about embodied perception and manipulation, with a focus on enabling robots to autonomously perceive, understand, and interact with the world.  
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>



        <!-- GenPose -->
        <tr>
          <td style="padding:10px;width:30%;vertical-align:middle">
            <img src='publications/2023_GenPose/GenPose_V2.gif' width="100%">
          </td>
          <td style="padding:20px;width:70%;vertical-align:middle">
            <span class="papertitle"><a href="https://sites.google.com/view/genpose">GenPose: Generative Category-level Object Pose Estimation via Diffusion Models</a></span>
            <br><strong>Jiyao Zhang</strong>*, Mingdong Wu*, Hao Dong‚Ä†<br>
            <em>Arxiv </em> 2023
            <br>
            <a href="https://arxiv.org/pdf/2306.10531.pdf">Arxiv</a> /
            <a href="https://sites.google.com/view/genpose">Project Page</a> / 
            <a href="https://github.com/Jiyao06/GenPose">Code</a> /
            <a href="javascript:hideshow(document.getElementById('genpose'))">Bibtex</a>
            <p id="genpose" style="font:1px; display: none">
              @article{zhang2023genpose,
                <br>&emsp;title={GenPose: Generative Category-level Object Pose Estimation via Diffusion Models},
                <br>&emsp;author={Zhang, Jiyao and Wu, Mingdong and Dong, Hao},
                <br>&emsp;journal={arXiv preprint arXiv:2306.10531},
                <br>&emsp;year={2023}
              }
            </p>
            <p></p>
            <p>We explore a pure generative approach to tackle the <strong>multi-hypothesis</strong> issue in 6D Category-level Object Pose Estimation. The key idea is to generate pose candidates using a score-based diffusion model and <strong>aggregate poses using an energy-based diffusion model</strong>. By aggregating the remaining candidates, we can obtain a robust and high-quality output pose.</p>
          </td>
        </tr>




        <tr>
          <td style="padding:10px;width:30%;vertical-align:middle">
            <img src='publications/2023_SGTAPose/SGTAPose.png' width="100%">
          </td>
          <td style="padding:20px;width:70%;vertical-align:middle">
            <span class="papertitle"><a href="https://sites.google.com/view/sgtapose">SGTAPose: Robot Structure Prior Guided Temporal Attention for Camera-to-Robot Pose Estimation from Image Sequence</a></span>
            <br>Yang Tian*, <strong>Jiyao Zhang</strong>*, Zekai Yin, Hao Dong‚Ä†<br>
            <em>Conference on Computer Vision and Pattern Recognition (CVPR) </em> 2023
            <br>
            <a href="https://openaccess.thecvf.com/content/CVPR2023/html/Tian_Robot_Structure_Prior_Guided_Temporal_Attention_for_Camera-to-Robot_Pose_Estimation_CVPR_2023_paper.html">Paper</a> /
            <a href="https://sites.google.com/view/sgtapose">Project Page</a> /
            <a href="https://github.com/Jiyao06/SGTAPose">Code</a> /
            <a href="javascript:hideshow(document.getElementById('sgtapose'))">Bibtex</a>
            <p id="sgtapose" style="font:1px; display: none">
              @inproceedings{tian2023robot,
                <br>&emsp;title={Robot Structure Prior Guided Temporal Attention for Camera-to-Robot Pose Estimation From Image Sequence},
                <br>&emsp;author={Tian, Yang and Zhang, Jiyao and Yin, Zekai and Dong, Hao},
                <br>&emsp;booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
                <br>&emsp;pages={8917--8926},
                <br>&emsp;year={2023}
              }
            <p></p>
            <p> We propose <strong>S</strong>tructure <strong>P</strong>rior <strong>G</strong>uided <strong>T</strong>emporal <strong>A</strong>ttention for online Camera-to-Robot <strong>Pose</strong> estimation (<strong>SGTAPose</strong>) from successive frames of an image sequence.</p>
          </td>
        </tr>




        <tr>
          <td style="padding:10px;width:30%;vertical-align:middle">
            <img src='publications/2022_DREDS/DREDS.gif' width="100%">
          </td>
          <td style="padding:20px;width:70%;vertical-align:middle">
            <span class="papertitle"><a href="https://pku-epic.github.io/DREDS/">Domain Randomization-Enhanced Depth Simulation and Restoration for Perceiving and Grasping Specular and Transparent Objects</a></span>
            <br>Qiyu Dai*, <strong>Jiyao Zhang</strong>*, Qiwei Li, Tianhao Wu, Hao Dong, Ziyuan Liu, Ping Tan, He Wang‚Ä†<br>
            <em>European Conference on Computer Vision (ECCV) </em> 2022
            <br>
            <a href="https://arxiv.org/abs/2208.03792">Paper</a> /
            <a href="https://pku-epic.github.io/DREDS/">Project Page</a> /
            <a href="https://github.com/PKU-EPIC/DREDS">Code</a> /
            <a href="javascript:hideshow(document.getElementById('dreds'))">Bibtex</a>
            <p id="dreds" style="font:1px; display: none">
              @inproceedings{dai2022domain,
                <br>&emsp;title={Domain randomization-enhanced depth simulation and restoration for perceiving and grasping specular and transparent objects},
                <br>&emsp;author={Dai, Qiyu and Zhang, Jiyao and Li, Qiwei and Wu, Tianhao and Dong, Hao and Liu, Ziyuan and Tan, Ping and Wang, He},
                <br>&emsp;booktitle={European Conference on Computer Vision},
                <br>&emsp;pages={374--391},
                <br>&emsp;year={2022},
                <br>&emsp;organization={Springer}
              }
            <p></p>
            <p>We propose <strong>D</strong>omain <strong>R</strong>andomization <strong>E</strong>nhanced <strong>D</strong>epth <strong>S</strong>imulation (<strong>DREDS</strong>) approach to simulate an active stereo depth system using physically based rendering and demonstrate that the proposed DREDS <strong>bridges the sim-to-real domain gap</strong>.</p>
          </td>
        </tr>




        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Template stolen from <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
